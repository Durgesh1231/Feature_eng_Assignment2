{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1: Min-Max Scaling and its Application:\n",
        "# Min-Max scaling, also known as normalization, transforms data to fit within a specific range, usually between 0 and 1.\n",
        "# Formula: X_scaled = (X - X_min) / (X_max - X_min)\n",
        "# It is commonly used when features have different scales, making them comparable for machine learning algorithms.\n",
        "# Example: Consider a dataset where the feature \"price\" ranges from 100 to 1000. Min-Max scaling will transform the \"price\"\n",
        "# values to fall within the range [0, 1] based on the minimum and maximum values in the dataset.\n",
        "\n",
        "# Q2: Unit Vector Technique vs. Min-Max Scaling:\n",
        "# The Unit Vector technique, also known as normalization or vector normalization, scales the feature vector to have a unit norm.\n",
        "# Formula: X_normalized = X / ||X||\n",
        "# In contrast, Min-Max scaling transforms the data to a specific range.\n",
        "# Example: For a dataset with features [2, 5, 7], the Unit Vector scaling will divide each feature by the vector's norm (magnitude),\n",
        "# whereas Min-Max scaling will transform the features to the [0, 1] range, based on the minimum and maximum values.\n",
        "\n",
        "# Q3: Principal Component Analysis (PCA) and Dimensionality Reduction:\n",
        "# PCA is a technique used to reduce the dimensionality of a dataset by transforming the data into a new set of orthogonal axes (principal components).\n",
        "# It preserves the variance and maximizes the information contained in fewer dimensions.\n",
        "# Example: In a dataset with many features (e.g., height, weight, age), PCA can combine correlated features into fewer principal components,\n",
        "# reducing the number of features while retaining most of the original data's variance.\n",
        "\n",
        "# Q4: PCA and Feature Extraction:\n",
        "# PCA is a method of feature extraction, where the goal is to create new features (principal components) that represent the most important information.\n",
        "# PCA can extract new features that capture the maximum variance in the data, helping with dimensionality reduction and improving model performance.\n",
        "# Example: In a facial recognition task, PCA can extract new features (principal components) from pixel values that best represent the variations\n",
        "# in faces, reducing the number of dimensions required for modeling while maintaining the key information.\n",
        "\n",
        "# Q5: Using Min-Max Scaling in a Food Delivery Recommendation System:\n",
        "# In a recommendation system, the dataset may contain features such as \"price,\" \"rating,\" and \"delivery time.\"\n",
        "# To preprocess this data, Min-Max scaling can be applied to standardize the features to a range [0, 1].\n",
        "# Example: Suppose \"price\" ranges from 5 to 50, \"rating\" from 1 to 5, and \"delivery time\" from 15 to 60 minutes.\n",
        "# Using Min-Max scaling, each feature will be normalized to fit within the range [0, 1], making it easier for the algorithm to compare and weigh these features.\n",
        "\n",
        "# Q6: Using PCA for Dimensionality Reduction in a Stock Price Prediction Model:\n",
        "# In stock price prediction, you may have a dataset with multiple features (e.g., financial data, market trends).\n",
        "# PCA can be applied to reduce the dimensionality by finding the principal components that explain the most variance in the data.\n",
        "# Example: If the dataset contains 50 features, PCA could reduce it to 10 principal components that explain 95% of the variance, helping to improve model performance by focusing on the most significant factors.\n",
        "\n",
        "# Q7: Min-Max Scaling Example for a Dataset [1, 5, 10, 15, 20]:\n",
        "# Given a range of -1 to 1, the Min-Max scaling formula is:\n",
        "# X_scaled = 2 * (X - X_min) / (X_max - X_min) - 1\n",
        "# Min = 1, Max = 20\n",
        "dataset = [1, 5, 10, 15, 20]\n",
        "X_min = min(dataset)\n",
        "X_max = max(dataset)\n",
        "scaled_values = [(2 * (x - X_min) / (X_max - X_min)) - 1 for x in dataset]\n",
        "scaled_values  # Output: [-1.0, -0.5, 0.0, 0.5, 1.0]\n",
        "\n",
        "# Q8: Feature Extraction Using PCA for [height, weight, age, gender, blood pressure]:\n",
        "# PCA will identify the principal components that capture the most variance in the features.\n",
        "# The number of principal components retained depends on the cumulative variance explained by each component.\n",
        "# If the first two principal components explain 95% of the variance, you would choose to retain those 2 components.\n",
        "# The goal is to keep enough components that capture most of the data's information while reducing dimensionality.\n"
      ]
    }
  ]
}